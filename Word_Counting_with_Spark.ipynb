{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RKJWSvSKOl1c"
   },
   "source": [
    "# PySpark Demo and Word Counting with Spark\n",
    "\n",
    "To get you started, we'll walk you through a bit of Colab specific Python and some PySpark code, and then we'll do the classic word count example, followd by some tasks for you to try.\n",
    "\n",
    "**Please run through the notebook cell by cell (using 'run' above or 'shift-return' on the keyboard).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "95B3FYvqPya6"
   },
   "source": [
    "##Preliminaries: Preparing Colab and Spark\n",
    "1.   When you open this notebook from the shared \"Data-Engineering\" folder, you don't have write acceess. When you save it, a copy will be created in the folder \"Colab Notebooks\".\n",
    "2.   The code below will mount Google Drive as a directory in the file system (the machine is a virtual Linux box). You will be asked to authorise this and provide an authentication code available through a link.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "8edGFcfkPx50",
    "outputId": "1b8d8ccd-8d6c-4876-8372-bd21c1ea5f9f"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-386ff909cd20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load the Drive helper and mount\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# This will prompt for authorization.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GpO-PaoGUID4"
   },
   "source": [
    "Next we move to the \"Colab Notebooks\" folder on your drive and create subfolder \"data\". Then we copy the \"hamlet.txt\" file there (you can check on Google Drive if it has worked). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "_Bn28SFjUHC_",
    "outputId": "2530e5ff-9f87-4fc7-d226-7692103d7df7"
   },
   "outputs": [],
   "source": [
    "# %cd /content/drive/My\\ Drive/\n",
    "# !mkdir \"Colab Notebooks\"\n",
    "# %cd \"Colab Notebooks\"\n",
    "# !mkdir data\n",
    "# %cd data\n",
    "# !cp \"/content/drive/My Drive/Data-Engineering/data/hamlet.txt\" .\n",
    "\n",
    "# instead of copying in the script above, upload the hamlet.txt file into the path above using drive\n",
    "# you will need to create the above folder structure\n",
    "\n",
    "# if you use a different naming scheme you will need to adjust the fileName as necessary\n",
    "# use the 'Files' tab of the menu that is hidden on the left of the screen to see the file system structure\n",
    "\n",
    "# find out the working directory\n",
    "!ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fpUd_fHJRInX"
   },
   "source": [
    "Next, we Install Spark (may take a while) and altair for visualisaion. This will need to be done every time a new machine is created. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "id": "2S9ShIHjSlDS",
    "outputId": "3571d07f-0e31-4d9f-b067-e342ecf42837"
   },
   "outputs": [],
   "source": [
    "# Install spark-related dependencies\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# !wget -q http://www-eu.apache.org/dist/spark/spark-2.3.3/spark-2.3.3-bin-hadoop2.7.tgz\n",
    "!wget -q http://www-eu.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
    "\n",
    "!pip install -q findspark\n",
    "!pip install pyspark\n",
    "# Set up required environment variables\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-9-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
    "#!pip install pyspark\n",
    "!pip install altair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lVuFYFEmjWjx"
   },
   "source": [
    "If the installation above doesnn't work, try the one below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rmJvtPxjOl1e"
   },
   "source": [
    "## Part 1 - Demo: Apapche Spark API with PySpark\n",
    "\n",
    "Basically there are 32APIs available in Apache Spark - RDD (Resilient Distributed Datasets) and DataFrame (extended by Dataset in Scala and Java). In this lab we will look at RDDs and Dataframes in Python.\n",
    "\n",
    "For more information on the Spark framework - visit (https://spark.apache.org)\n",
    "For more information on the Pyspark API - visit (https://spark.apache.org/docs/latest/api/python/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IdIBCxgFgFQT",
    "outputId": "a057f65c-0ca2-4090-b665-d60f0a6ab7f0"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zmoNsdqQOl1f"
   },
   "source": [
    "### 1) Access to Spark\n",
    "\n",
    "We start by cretaing a SparkContext, normally called `sc`. \n",
    "We use that to create RDDs and a SparkSession object (for DataFrames), often just called `spark`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "12aW4ncAOl1h",
    "outputId": "9781477b-6d54-41d0-93c6-2dcaad63181f"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "# get a spark context\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "print(sc)\n",
    "# get the context\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "print(spark) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ctb3qeAkOl1m"
   },
   "source": [
    "### 2) RDD Creation\n",
    "\n",
    "There are two ways to create RDDs. The first is to parallelise a Python object that exists in your driver process (i.e. this one). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jbuuRZv6Ol1n"
   },
   "source": [
    "The second way is to create an RDD is by referencing an external dataset such as a shared filesystem, HDFS, HBase, or just data source offering a Hadoop InputFormat. This is what we will be using in this lab (further down)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "phwtMBs2Ol1p",
    "outputId": "d35399ab-ea8f-4960-b638-815c571bbd15"
   },
   "outputs": [],
   "source": [
    "# Creat an RDD from a Python object in this process (the \"driver\").\n",
    "# The parallelize function  creating the \"numbers\" RDD\n",
    "data = [1,2,3,4,5]\n",
    "firstRDD = sc.parallelize(data)\n",
    "print(firstRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sx75RI9mOl1s"
   },
   "source": [
    "This RDD lives now on as many worker machines as are available and as are deemed useful by Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cDnZCdSTOl1t"
   },
   "source": [
    "### 3) RDD operations\n",
    "RDDs have two kinds of operations: *Transformations* and *Actions*.\n",
    "\n",
    "*Transformations* create a new RDD by applying a function to the items in the RDD. The function will be remembered, but only be applied when needed (\"*lazy* evaluation\").\n",
    "\n",
    "*Actions* produce some output from the data. An *Action* will trigger the execution of all *Transformations*.\n",
    "\n",
    "Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Q25kDkt9Ol1v",
    "outputId": "6931010a-57d5-4227-d775-def2034dbfb6"
   },
   "outputs": [],
   "source": [
    "def myfun(x):\n",
    "  return x+3\n",
    "# lambda function: x -> x+3\n",
    "#RDD2 = firstRDD.map(lambda x:x+3)  \n",
    "RDD2 = firstRDD.map(myfun)  \n",
    "print(RDD2)\n",
    "# nothing happened to far, as there is no action\n",
    "#RDD2.count()\n",
    "RDD2.countApprox(10000, 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QhIIytM9bSvm"
   },
   "source": [
    "If the functions are short (one expression, to be exact), this is more convenient write with a lamba expression, that creates an anonymous function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IEtzNnm4bZfe",
    "outputId": "7d357fd0-7fa8-4675-bbdf-8e39b02b60a3"
   },
   "outputs": [],
   "source": [
    "RDD3 = firstRDD.map(lambda x:x+3) # this is the same as using myfun \n",
    "print(RDD3)\n",
    "# nothing happened to far, as there is no action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ju4PvD5gOl10",
    "outputId": "5386dac2-79fa-4234-a435-65eebbd3ad7e"
   },
   "outputs": [],
   "source": [
    "# \"count\" is an action and triggers the transformation   \n",
    "a = RDD2.count() \n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6xp2aTh8Ol15"
   },
   "source": [
    "`collect` is an action that returns the values of the RDD in an Python array, back into this local driver process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "tdaNRc0SOl15",
    "outputId": "631cef9a-1746-4dec-a642-580011e4e10f"
   },
   "outputs": [],
   "source": [
    "a = RDD2.collect() \n",
    "print(a)\n",
    "\n",
    "b = RDD3.collect() \n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FXmtXG0hOl19"
   },
   "source": [
    "As we can seee above, *myfun* (RDD2) and the *lambda x: x+3* (RDD3) have the same effect.\n",
    "\n",
    "Look here for more information about the functions provided by the RDD class: (https://spark.apache.org/docs/2.4.0/api/python/pyspark.html#pyspark.RDD). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lUl3upk0Ol1-"
   },
   "source": [
    "### 4) Dataframes \n",
    "\n",
    "Dataframes are a more structured form of storage than RDDs and similar to Pandas dataframes.  \n",
    "\n",
    "Let us see how to create and use dataframes. There are three ways of creating a dataframe\n",
    "    a) from an existing RDD.\n",
    "    b) form and external data source, e.g., loading the data from JSON or CSV files.\n",
    "    c) Programmatically specifying schema and data.\n",
    "    \n",
    "Here is an example for option a). We use the *Row* class to create structure data rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hSIzFqAQOl1_",
    "outputId": "034c768c-4967-4f2b-a4a7-49270071170c"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "dataList = [('Anne',21),('Bob',22),('Carl',29),('Daisy',36)] # our data as a list\n",
    "rdd = sc.parallelize(dataList) # RDD from the list\n",
    "peopleRDD = rdd.map(lambda x: Row(name=x[0], age=int(x[1]))) # RDD\n",
    "peopleDF = spark.createDataFrame(peopleRDD) \n",
    "print(peopleDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vsGAOxX2Ol2G"
   },
   "source": [
    "## Part 2: Classic Word Count\n",
    "\n",
    "We will now do the classic word count example for the MapReduce pattern.\n",
    "\n",
    "We will apply it to the text of Sheakespeare's play *Hamlet*. For that you should have uploaded the file \"hamlet.txt\" into the data assets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-eWjGlkbOl2H"
   },
   "source": [
    "### 6) Load the data\n",
    "First we need to load the text into an RDD (the second method of creating an RDD as mentioned above). \n",
    "\n",
    "We need to specify the path, and we can read directly from the shared Data-Engineering directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kzdsuhxtOl2H"
   },
   "outputs": [],
   "source": [
    "# filepath = \"/content/drive/My Drive/Data-Engineering/data/hamlet.txt\"\n",
    "# use a relative path to the file\n",
    "filepath = \"drive/My Drive/Data-Engineering/data/hamlet.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "1uG51YrzlH7a",
    "outputId": "38156f40-05d1-49bd-bc6e-7f6684df2d16"
   },
   "outputs": [],
   "source": [
    "# verify that the file is there\n",
    "!ls -l \"drive/My Drive/Data-Engineering/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_5q8UiCtOl2J"
   },
   "source": [
    "You can read the file into an RDD with `textFile`. The RDD then contains as items the lines of the text. `take(3)` then gives us the first 3 lines.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "OM_vQDsGOl2J",
    "outputId": "4bd6466b-bc80-4275-e2e3-e8562c518e87"
   },
   "outputs": [],
   "source": [
    "lineRDD = sc.textFile(filepath)\n",
    "lineRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ab6vKwgvOl2L"
   },
   "source": [
    "### 7) Split lines into words\n",
    "\n",
    "In order to count the words, we need to split the lines into words. We can do that using the `split` function of the Python String class to separate at each space. \n",
    "\n",
    "The map function replaces each item with a new one, in this case our `lambda` returns an array of words (provided by `split(' ')`). However, we want to create one item per word, therefore we need to use a function called `flatMap` that creates a new RDD item for every item in the array returned by the lambda.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WwI2cDlSOl2L",
    "outputId": "6a1300c4-56b6-4a75-99ca-db4dad4b4f25"
   },
   "outputs": [],
   "source": [
    "wordRDD = lineRDD.flatMap(lambda x: x.split(' '))\n",
    "wordRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jDuHh_ocOl2N"
   },
   "source": [
    "Map the words to tuples of the form *(word, 1)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0PXZXAGpOl2O",
    "outputId": "4af8a9c7-ba4f-4e7f-c5cf-ed54b5503ef4"
   },
   "outputs": [],
   "source": [
    "word1RDD = wordRDD.map(lambda x: (x, 1))\n",
    "word1RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5BplN07GOl2P"
   },
   "source": [
    "### 8) Count by reducing\n",
    "For Spark, the first part in each tuple is the 'key'. Now we can use reduceByKey() to add the 1s and get the number of occurences per word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HRFaQARsOl2Q",
    "outputId": "c3cac69e-da23-45ac-e922-22cf74926197"
   },
   "outputs": [],
   "source": [
    "wordCountRDD = word1RDD.reduceByKey(lambda x,y: x+y )\n",
    "wordCountRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G38rUrlVOl2S"
   },
   "source": [
    "### 9) Filtering \n",
    "\n",
    "There are many empty strings returned by the splitting. We can remove them by filtering.\n",
    "Then can take a shortcut and use a ready-made functions 'count by value', which does the same as we before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZQ9mBefpOl2S",
    "outputId": "b31ed6fb-409f-4717-d809-7f7cdc8e5721"
   },
   "outputs": [],
   "source": [
    "wordFilteredRDD = wordRDD.filter(lambda x: len(x)>0)\n",
    "word1RDD = wordFilteredRDD.map(lambda x: (x, 1))\n",
    "wordCountRDD = word1RDD.reduceByKey(lambda x,y: x+y )\n",
    "wcList = wordCountRDD.take(5)\n",
    "print(wcList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oyubLNalOl2W"
   },
   "source": [
    "## Part 3: Tasks for you to work on\n",
    "\n",
    "Based on the examples above, you can now try and write some code yourself.  Look for the lines starting with **>>>**. You neeed to fix them by writing your own code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4LY4f9UwOl2W"
   },
   "source": [
    "## Task 1) Better splitting \n",
    "\n",
    "Currently our 'words' can contain punctuation, becausee only spaces are removed. A better way to split is using regular expressions  (Python's 're' package)(https://docs.python.org/3.5/library/re.html?highlight=regular%20expressions). `re.split('\\W+', 'my. test. string!')` does a good job. Try it out below by fixing the line that starts with '>>>'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wuQDbhOAOl2X",
    "outputId": "f494d2dd-f2ce-4d5f-c365-482bc0369c44"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "#>>> wordRDD = lineRDD.flatMap(lambda x: ...)\n",
    "wordRDD = lineRDD.flatMap(lambda x: re.split('\\W+', x)) # apply re.split('\\W+', string) here\n",
    "wordFilteredRDD = wordRDD.filter(lambda x: len(x)>0) # filtering\n",
    "wordFilteredRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ktBnkI5TOl2Y"
   },
   "source": [
    "## 2) Use lower case\n",
    "\n",
    "Convert all strings to lower case (using `.lower()` provided by the Python string class), so that 'Test' and 'test' count as the same. Package it into one a tuple of the form (word,1) in the same call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DBqa2EQjOl2Y",
    "outputId": "18feeace-fdf4-4fd6-f659-bc03e2fd7255"
   },
   "outputs": [],
   "source": [
    "#>>> wordLowerRDD = wordFilteredRDD.map(lambda x: ... )\n",
    "wordLowerRDD = wordFilteredRDD.map(lambda x: x.lower() )\n",
    "wordLowerRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "WIVwV56lOl2a",
    "outputId": "99684437-0a80-4951-955c-713701540e63"
   },
   "outputs": [],
   "source": [
    "word1RDD = wordLowerRDD.map(lambda x: (x,1)) # we can now get better word count results\n",
    "wordCountRDD = word1RDD.reduceByKey(lambda x,y: x+y) # we can now get better word count results\n",
    "wordCountRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7jgnYSVOl2b"
   },
   "source": [
    "## 3) Filter rare words\n",
    "\n",
    "Add a filtering step call remove all words with less than 5 occurrences. This can be useful to identify common topics in documents, where very rare words can be misleading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "CTd9S_y6Ol2b",
    "outputId": "381692d1-23c3-4ac2-aa8f-c3ae7c018724"
   },
   "outputs": [],
   "source": [
    "# the trick here is to apply the lambda only to the second part of each item, i.e. x[1] \n",
    "#freqWordsRDD = wordCountRDD.filter(lambda x:  ... ) # tip: filter keeps the times where the lambda returns true.\n",
    "freqWordsRDD = wordCountRDD.filter(lambda x:  x[1]>5 ) # tip: filter keeps the times where the lambda returns true.\n",
    "freqWordsRDD.take(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IypcO9SPOl2d"
   },
   "source": [
    "## 4) List only stopwords\n",
    "\n",
    "Stopwords are frequent words that are not topic-specifc.  Stopwords can be useful for recognising the style of an author. Removing stopwords can be useful in regocnising the topic of a document. \n",
    "\n",
    "Below is a small list of stopwords. Filter the tuples where the first part is a stopword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "T85Nx6ZuOl2e",
    "outputId": "a8c3e075-ef20-4aa6-b621-ed8cf74512a1"
   },
   "outputs": [],
   "source": [
    "stopWordList = ['the','a','in','of','on','at','for','by','I','you','me'] \n",
    "#stopWordsRDD = freqWordsRDD.filter(lambda x:  ...) # the 1st part of the tuple should be in the list\n",
    "stopWordsRDD = freqWordsRDD.filter(lambda x: x[0] in stopWordList ) # the 1st part of the tuple should be in the list \n",
    "\n",
    "stopWordsRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-rysKdLffGTn",
    "outputId": "380228a1-5380-42e5-8a5a-83f21e03bc73"
   },
   "outputs": [],
   "source": [
    "stopWordsRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gKVS18B9Ol2f"
   },
   "source": [
    "There are only a few words, so we can see the vies results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "JsX57MXHOl2g",
    "outputId": "3f6f4057-6a53-432c-d9d8-ff4a7b8f1190"
   },
   "outputs": [],
   "source": [
    "output = stopWordsRDD.collect() \n",
    "for (word, count) in output:\n",
    "    print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJcV-ulGOl2i"
   },
   "source": [
    "We can now visualise the stopword counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "Y3p___CROl2i",
    "outputId": "5fe5a2d9-a4c4-46b6-fbc1-6c92936352ac",
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "handlerId": "barChart",
      "keyFields": "_1",
      "rowCount": "500",
      "valueFields": "_2"
     }
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "word_list=[]\n",
    "item_list=[]\n",
    "for item in output:\n",
    "  (word,count)=item\n",
    "  word_list.append(word)\n",
    "  item_list.append(count)\n",
    "df3=pd.DataFrame({'words':word_list,'items':item_list})\n",
    "df3"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Revised Word_Counting_with_Spark - Solutions.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
